# CloudWatch Logs Insights Queries

This document provides a collection of useful CloudWatch Logs Insights queries for analyzing logs from the Experimentation Platform.

## Overview

[CloudWatch Logs Insights](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html) is a powerful tool for querying and analyzing your logs. The queries below will help you extract valuable information from the logs generated by the error tracking and monitoring system.

## Error Analysis Queries

### Find Most Frequent Errors

```
fields @timestamp, error_type, error_message
| parse message "\"error_type\":\"*\"" as error_type
| parse message "\"error_message\":\"*\"" as error_message
| stats count(*) as error_count by error_type, error_message
| sort error_count desc
| limit 20
```

### Errors By Endpoint

```
fields @timestamp, @message
| parse message "\"url\":\"*\"" as url
| parse message "\"error_type\":\"*\"" as error_type
| stats count(*) as error_count by url, error_type
| sort error_count desc
| limit 20
```

### View Complete Error Details for Debugging

```
fields @timestamp, @message
| parse message "\"error_type\":\"*\"" as error_type
| filter error_type = "ValueError"
| sort @timestamp desc
| limit 10
```

*Replace "ValueError" with the error type you're investigating.*

### Errors by Time of Day

```
fields @timestamp
| parse message "\"error_type\":\"*\"" as error_type
| stats count(*) as error_count by bin(1h)
| sort @timestamp asc
```

## Performance Analysis Queries

### High Latency Requests

```
fields @timestamp, @message
| filter message like "Request metrics"
| parse message "\"latency_ms\":*," as latency_ms
| parse message "\"endpoint\":\"*\"" as endpoint
| filter latency_ms > 1000
| sort latency_ms desc
| limit 20
```

### Average Latency by Endpoint

```
fields @timestamp, @message
| filter message like "Request metrics"
| parse message "\"latency_ms\":*," as latency_ms
| parse message "\"endpoint\":\"*\"" as endpoint
| stats avg(latency_ms) as avg_latency, min(latency_ms) as min_latency, max(latency_ms) as max_latency, count(*) as request_count by endpoint
| sort avg_latency desc
```

### CPU Usage Spikes

```
fields @timestamp, @message
| filter message like "Request metrics"
| parse message "\"cpu_percent\":*," as cpu_percent
| filter cpu_percent > 80
| sort @timestamp desc
| limit 20
```

### Memory Usage Patterns

```
fields @timestamp, @message
| filter message like "Request metrics"
| parse message "\"memory_mb\":*," as memory_mb
| stats avg(memory_mb) as avg_memory, max(memory_mb) as max_memory by bin(5m)
| sort @timestamp asc
```

## User and Request Analysis

### Requests by Client IP

```
fields @timestamp, @message
| parse message "\"host\":\"*\"" as client_ip
| stats count(*) as request_count by client_ip
| sort request_count desc
| limit 20
```

### User Agent Analysis

```
fields @timestamp, @message
| parse message "\"user-agent\":\"*\"" as user_agent
| stats count(*) as request_count by user_agent
| sort request_count desc
| limit 20
```

### Request Volume by Time

```
fields @timestamp, @message
| filter message like "Request metrics"
| stats count(*) as request_count by bin(5m)
| sort @timestamp asc
```

## Combined Analysis

### Correlation Between CPU and Latency

```
fields @timestamp, @message
| filter message like "Request metrics"
| parse message "\"latency_ms\":*," as latency_ms
| parse message "\"cpu_percent\":*," as cpu_percent
| parse message "\"endpoint\":\"*\"" as endpoint
| stats avg(latency_ms) as avg_latency, avg(cpu_percent) as avg_cpu by endpoint
| sort avg_cpu desc
```

### Identifying Slow Endpoints with High Resource Usage

```
fields @timestamp, @message
| filter message like "Request metrics"
| parse message "\"latency_ms\":*," as latency_ms
| parse message "\"cpu_percent\":*," as cpu_percent
| parse message "\"memory_mb\":*," as memory_mb
| parse message "\"endpoint\":\"*\"" as endpoint
| stats avg(latency_ms) as avg_latency, avg(cpu_percent) as avg_cpu, avg(memory_mb) as avg_memory, count(*) as request_count by endpoint
| sort avg_latency desc
| limit 10
```

## Using These Queries

1. Go to the AWS CloudWatch console
2. Select "Logs Insights" from the left navigation
3. Select the log group(s) to query:
   - `/experimentation-platform/api`
   - `/experimentation-platform/errors`
   - `/experimentation-platform/services`
4. Copy and paste the query
5. Adjust the time range as needed
6. Click "Run query"

## Creating Custom Dashboards from Queries

You can save your queries and add them to CloudWatch dashboards:

1. Run your query
2. Click "Save"
3. Enter a name for the query
4. Click "Save query"
5. Click "Add to dashboard"
6. Select an existing dashboard or create a new one
7. Configure the widget
8. Click "Add to dashboard"

## Exporting Query Results

To export query results:

1. Run your query
2. Click "Export results"
3. Choose "CSV" or "JSON" format
4. Save the file to your computer

## Optimizing Queries

- Limit the time range to improve performance
- Use specific field selectors rather than scanning the entire message
- Use filters early in the query to reduce the data being processed
- Use the `stats` command to aggregate data instead of returning raw logs
- Consider using a sampling approach for high-volume logs

## Troubleshooting

If your queries aren't returning expected results:

1. Check the log format to ensure parse patterns match
2. Verify the field names in your parse statements
3. Start with a simple query and gradually add complexity
4. Use the `fields @message` command to see raw log messages
5. Check the time range to ensure it covers the period you're interested in
